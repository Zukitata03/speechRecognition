{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa21d5991e84a4cac55b60b4a06b2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Record', icon='microphone', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa095171749b421b9596929a163d73a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='Stop', icon='pause', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c66f0885e49ef9f26597c49c84e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Analyze', icon='stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73fac9e7c4b40d79d10f0da6620ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she doesn't love me</td>\n",
       "      <td>Negative ðŸ˜ž</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TEXT     Emotion\n",
       "0  she doesn't love me  Negative ðŸ˜ž"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from IPython.display import display\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "messages = Queue() #tell the thread when to stop recording\n",
    "recordings = Queue() #store the recordings\n",
    "\n",
    "\n",
    "transcriptions = []\n",
    "transcribe = None\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "record_button = widgets.Button(\n",
    "    description='Record',\n",
    "    disabled=False,\n",
    "    button_style = 'success',\n",
    "    icon = 'microphone'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style = 'warning',\n",
    "    icon = 'pause'\n",
    ")\n",
    "\n",
    "\n",
    "def start_recording(data):\n",
    "    messages.put(True)\n",
    "    \n",
    "    with output:\n",
    "        display(\"Recording...\")\n",
    "        record = Thread(target=record_microphone)\n",
    "        record.start()\n",
    "        \n",
    "        transcribe = Thread(target=speech_recognition, args=(output,))\n",
    "        transcribe.start()\n",
    "        \n",
    "        \n",
    "def stop_recording(data):\n",
    "    global transcriptions, transcribe\n",
    "    with output:\n",
    "        messages.get()\n",
    "        display(\"Stopped recording\")\n",
    "        \n",
    "        # Wait for the transcription thread to finish\n",
    "        if transcribe is not None:\n",
    "            transcribe.join()\n",
    "            \n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    # Create a DataFrame from all transcriptions and save to CSV\n",
    "    if transcriptions:  # Make sure the list is not empty\n",
    "        df = pd.DataFrame(transcriptions, columns=['TEXT'])\n",
    "        df.to_csv('all_transcriptions.csv', index=False)\n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        df['Emotion'] = df['TEXT'].apply(getSentiment)\n",
    "\n",
    "        # Save updated DataFrame to CSV\n",
    "        df.to_csv('all_transcriptions.csv', index=False)\n",
    "        \n",
    "        # Style DataFrame\n",
    "        styled = df.style.set_table_styles([\n",
    "            {\"selector\": \"th\", \"props\": [(\"background-color\", \"white\")]},\n",
    "            {\"selector\": \"td\", \"props\": [(\"background-color\", \"white\")]}\n",
    "        ])\n",
    "\n",
    "        display(styled)\n",
    "    else:\n",
    "        display(\"No transcriptions to analyze\")\n",
    "        \n",
    "\n",
    "analyze_button = widgets.Button(\n",
    "    description=\"Analyze\",\n",
    "    disabled=False,\n",
    "    button_style = 'danger',\n",
    "    icon = 'stop'\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "#Click xong thÃ¬ nÃ³ sáº½ gá»i hÃ m start_recording vÃ  stop_recording\n",
    "record_button.on_click(start_recording) \n",
    "stop_button.on_click(stop_recording)    \n",
    "analyze_button.on_click(analyze_sentiment)\n",
    "\n",
    "output = widgets.Output()\n",
    "display(record_button, stop_button, analyze_button, output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Input', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 1, 'structVersion': 2, 'name': 'Microphone Array (4- Realtek(R)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 2, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Output', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 3, 'structVersion': 2, 'name': 'FxSound Speakers (FxSound Audio', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 4, 'structVersion': 2, 'name': 'Speakers (4- Realtek(R) Audio)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 5, 'structVersion': 2, 'name': 'Primary Sound Capture Driver', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "{'index': 6, 'structVersion': 2, 'name': 'Microphone Array (4- Realtek(R) Audio)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "{'index': 7, 'structVersion': 2, 'name': 'Primary Sound Driver', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "{'index': 8, 'structVersion': 2, 'name': 'FxSound Speakers (FxSound Audio Enhancer)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "{'index': 9, 'structVersion': 2, 'name': 'Speakers (4- Realtek(R) Audio)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "{'index': 10, 'structVersion': 2, 'name': 'Speakers (4- Realtek(R) Audio)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "{'index': 11, 'structVersion': 2, 'name': 'FxSound Speakers (FxSound Audio Enhancer)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "{'index': 12, 'structVersion': 2, 'name': 'Microphone Array (4- Realtek(R) Audio)', 'hostApi': 2, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 48000.0}\n",
      "{'index': 13, 'structVersion': 2, 'name': 'Speakers (Realtek HD Audio output)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 14, 'structVersion': 2, 'name': 'Stereo Mix (Realtek HD Audio Stereo input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 15, 'structVersion': 2, 'name': 'Microphone Array (Realtek HD Audio Mic input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 16, 'structVersion': 2, 'name': 'Headphones (Realtek HD Audio 2nd output)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 17, 'structVersion': 2, 'name': 'Mic in at front panel (black) (Mic in at front panel (black))', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 18, 'structVersion': 2, 'name': 'Input ()', 'hostApi': 3, 'maxInputChannels': 8, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "{'index': 19, 'structVersion': 2, 'name': 'Speakers ()', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i))\n",
    "    \n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 1\n",
    "FRAME_RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "SAMPLE_SIZE = 2\n",
    "\n",
    "def record_microphone(chunk = 1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    stream = p.open(format=AUDIO_FORMAT, channels=CHANNELS, rate=FRAME_RATE, input=True,input_device_index=1, frames_per_buffer=chunk)\n",
    "    frames = []\n",
    "    \n",
    "    while not messages.empty():\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "        \n",
    "        #every record_seconds seconds, put the frames into the queue, then start recording another record_seconds audio \n",
    "        if len(frames) >= (FRAME_RATE * RECORD_SECONDS/chunk):\n",
    "            recordings.put(frames.copy())\n",
    "            frames = []\n",
    "            \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "model = Model(r'E:\\Homework\\AI\\vosk-model-small-en-us-0.15\\vosk-model-small-en-us-0.15')\n",
    "rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "rec.SetWords(True)\n",
    "\n",
    "def speech_recognition(output):\n",
    "    global transcriptions  # Declare transcriptions as global\n",
    "    while not messages.empty():\n",
    "        frames = recordings.get()\n",
    "        \n",
    "        rec.AcceptWaveform(b''.join(frames))\n",
    "        result = rec.Result()\n",
    "        text = json.loads(result)['text']\n",
    "        output.append_stdout(text)\n",
    "        transcriptions.append(text)  # Save transcription\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getSentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        return 'Positive ðŸ˜ƒ'\n",
    "    elif sentiment['compound'] <= -0.05:\n",
    "        return 'Negative ðŸ˜ž'\n",
    "    else:\n",
    "        return 'Neutral ðŸ˜'\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
